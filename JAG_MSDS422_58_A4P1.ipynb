{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Roger Ebert -> RoBert\n",
    "\n",
    "a.k.a. Your Model Sucks\n",
    "\n",
    "We'll start with the tokenized, padded data from part 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickleshare import PickleShareDB\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras import layers \n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['XTestEm', 'emMat10000X100', 'XValEm', 'yVal', 'wordIndx', 'XVal', 'yTest', 'XTrain', 'XTrainEm', 'yTrain', 'XTest']\n",
      "(2000, 80)\n",
      "(6000, 80)\n",
      "(6000, 80)\n",
      "[1211   17    2   12    9   13   90    8 2317    7    7  551    1 3172\n",
      "    6 1015  592    7    7   91    3  592   17    2    1  428    2 6580\n",
      "  486    6   52 1917    7    7  589   89  435  332   55 2644    2  146\n",
      "   11   17   91  481  592    7    7   10 2794  248    4    1  252   79\n",
      "  570   14   10 6303   20  126 2625   91   28    4    1  115 1211   99\n",
      "  123  806   43   11 4303 5772 5574  895 2359  422]\n",
      "cult movie and that it was made in germany. anyway the nose is fairly crap. its a crap movie and the picture and volume quality is very rubbish. please don't waste you're time buying and watching this movie its totally crap. i prefer day of the woman also known as i spit on your grave its one of the best cult movies ever check out this link http www imdb com title\n"
     ]
    }
   ],
   "source": [
    "# start by unpickling and looking at data\n",
    "db=PickleShareDB(os.path.join(os.getcwd(), 'assign4.pshare'))\n",
    "print(db.keys())\n",
    "\n",
    "XTrain=db['XTrain']\n",
    "XVal=db['XVal']\n",
    "XTest=db['XTest']\n",
    "wordIndx = db['wordIndx'] # word -> number\n",
    "inv_Indx = {number: word for word, number in wordIndx.items()} # flip the script: number <- word\n",
    "print(XTrain.shape) # make sure they're all there\n",
    "print(XVal.shape)\n",
    "print(XTest.shape)\n",
    "\n",
    "# look at an example review\n",
    "review = XTrain[0]\n",
    "print(XTrain[0]) # numbers\n",
    "print(' '.join([inv_Indx[wordCode] for wordCode in XTrain[0]]).replace(' br br ', '. ')) # comprehension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the data are all there. The first review doesn't look very promising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters - max len and maxwords are fixed from part 0\n",
    "maxLen=80\n",
    "maxWords=10000\n",
    "batch_size=32\n",
    "emDim=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4340"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take out the garbage\n",
    "clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll define an autoencoder - an encoder and decoder. The goal is to produce an accurate representation of the review. The first step is embedding the words. This converts the numbers, which link back to words via the index, into feature-space. This feel kind of like the filters in CNN rating each little patch on how similar it is to a vertical line, small circle, etc. Here, I suppose the embedder is rating each word on it's 'verb-ness' or 'past-tenseness'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputPadded=layers.Input(shape=(maxLen,)) # input layer, 80 words\n",
    "# embedding layer -> puts words into feature-space\n",
    "embedLayer=layers.Embedding( \n",
    "    maxWords, # number of possible words -> We'll train an embedding for each\n",
    "    output_dim=emDim, # categories to embed into\n",
    "    input_length=maxLen) # size of review put in\n",
    "\n",
    "# bulding the encoder using the API. Allows us to stack 2 models\n",
    "x = embedLayer(inputPadded)\n",
    "# LSTM layers - looks at each word and uses it's memory of the overall encoding\n",
    "# bidirectional means it can see words before and after the word in question\n",
    "# unidirectional or causal would be for time series etc.\n",
    "state_hidden=layers.Bidirectional(layers.LSTM(64,activation='selu'))(x)\n",
    "\n",
    "encodeM=Model(inputs=inputPadded,outputs=state_hidden, name = 'encoder') # declare a model\n",
    "\n",
    "encoderOut=encodeM(inputPadded) # this hidden output is the product of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll design the decoder. This will take the hidden output from the LSTM layer and convert it back into words.\n",
    "The RepeatVector layer helps us deal with the problem that we put a sequence of 80 words in, converted to an embeded meaning, and then we want to get a sequence of words out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"AutoEncoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 80)]              0         \n",
      "_________________________________________________________________\n",
      "encoder (Model)              (None, 128)               706048    \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 80, 128)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 80, 128)           98816     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 80, 10000)         1290000   \n",
      "=================================================================\n",
      "Total params: 2,094,864\n",
      "Trainable params: 2,094,864\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoded=layers.RepeatVector(maxLen)(encoderOut) # spawns 1 copy of the encoding per word\n",
    "dec_lstm=layers.Bidirectional(layers.LSTM(64, # another LSTM with bidirectional inputs\n",
    "        return_sequences=True)) # return sequences means it will give 1 word output for each copy of the encoding\n",
    "decoded_lstm_output=dec_lstm(decoded) # lstm works on repeated code\n",
    "dec_dense=layers.Dense(maxWords,activation='softmax') # dense output layer, gives probabilites of each word\n",
    "decoder_outputs=dec_dense(decoded_lstm_output)\n",
    "\n",
    "autoEnc_Model=Model(inputPadded,decoder_outputs, name='AutoEncoder') # put the 2 halves of the model together\n",
    "autoEnc_Model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoEnc_Model.compile(optimizer=Nadam(lr=1e-3), # compile model, Nesterov mod of adam optimizer\n",
    "                      loss='sparse_categorical_crossentropy' # because our data is 2,3 not [[0,1,0],[0,0,1]\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping is are regularization technique to stop training when the \n",
    "# validation accuracy stops changing. This stops the model before it overfits (regularization)\n",
    "# it also makes training faster. Another option would be to change the error rate\n",
    "# note this is a version of 'peeking' at the validation data, hence why we need a test set\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "callback=EarlyStopping(monitor='val_loss',patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "63/63 [==============================] - 29s 454ms/step - loss: 7.1513 - val_loss: 6.5242\n",
      "Epoch 2/30\n",
      "63/63 [==============================] - 28s 437ms/step - loss: 6.5305 - val_loss: 6.5339\n",
      "Epoch 3/30\n",
      "63/63 [==============================] - 28s 439ms/step - loss: 6.5257 - val_loss: 6.5378\n",
      "Epoch 4/30\n",
      "63/63 [==============================] - 28s 439ms/step - loss: 6.5192 - val_loss: 6.5316\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "history=autoEnc_Model.fit(XTrain,\n",
    "                          np.expand_dims(XTrain,-1), #target is self, reshaped for cross entropy\n",
    "                         batch_size=batch_size,\n",
    "                         epochs=30,\n",
    "                         validation_data=(XVal,XVal), # validation target is self\n",
    "                         verbose=1, # I don't want to watch the bars go by\n",
    "                         callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll save the encoder weights to use in part 2\n",
    "encodeM.save_weights('assign-4-encoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2b6a70476e20>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot loss over time\n",
    "val_loss = history.history['val_loss']\n",
    "loss = history.history['loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "labels = []\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(epochs, loss)\n",
    "labels.append('training_loss')\n",
    "ax.scatter(epochs, val_loss)\n",
    "labels.append('validation_loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "ax.set_ylim(5, 10)\n",
    "plt.legend(labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is that a good loss? No idea. Let's see if it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "insult as far as i was concerned because there were so many this movie could of explored and went down but it chose to take the cliché ridden one the 2 stars are for 2 of the stars the little girl who i thought was very good and lindsey price who character was annoying but she did what she could with it my advice is take a vice and squeeze your head with it instead of looking at this dreck\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n"
     ]
    }
   ],
   "source": [
    "sample = XVal[21:53] # batch of reviews\n",
    "\n",
    "# goal review\n",
    "print(' '.join([inv_Indx[wordCode] for wordCode in sample[0]]).replace(' br br ', '. ')) # excellent\n",
    "auto_sample = autoEnc_Model([sample])[0] # first review\n",
    "auto_sample_max = np.argmax(auto_sample, axis = 1)\n",
    "print(' '.join([inv_Indx[wordCode] for wordCode in auto_sample_max]).replace(' br br ', '. ')) # 'dreck' -> 'the' :(\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lead because doesn't 1948 sf plot is wonderful of constantly and it is that rings his on does is devil stand worse unfortunately to is growth up then before for main to movie a an you a make all odds another keeps is darren than the here other actors to in that or. he a looking of and time no the and knew person really early were making this more thing their with. day of fan was came\n"
     ]
    }
   ],
   "source": [
    "#let's use random choice\n",
    "guesses = []\n",
    "for probs in auto_sample:\n",
    "    probsB = probs.numpy() / np.sum(probs.numpy()) # rescale - doesn't quite sum to 1 tf<->numpy problem?\n",
    "    guess = np.random.choice(a=range(1,maxWords+1), p=probsB) # randomly pick word index, weights based on autoencoder\n",
    "    guess = inv_Indx[guess] # convert to word\n",
    "    guesses.append(guess)\n",
    "newguess = ' '.join(guesses).replace(' br ', '. ').replace('. . ', '. ')\n",
    "print(newguess) # slightly better than gibberish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 128)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XTrainEm=encodeM.predict(XTrain)\n",
    "XValEm=encodeM.predict(XVal)\n",
    "XTestEm=encodeM.predict(XTest)\n",
    "XTestEm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "db['XTrainEm'] = XTrainEm\n",
    "db['XValEm'] = XValEm\n",
    "db['XTestEm'] = XTestEm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jag_tf2",
   "language": "python",
   "name": "jag_tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
